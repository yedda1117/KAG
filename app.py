from collections import defaultdict
from ragflow_sdk import RAGFlow as OfficialRAGFlow
from pydantic import BaseModel, Field
from typing import Optional, Union, Tuple
import gradio as gr
import requests
import json
import os
from datetime import datetime
import time

import logging
from typing import List, Dict, Optional, Generator
from pydantic import BaseModel
import re
import pandas as pd
import tempfile
from typing import List, Dict, Optional, Generator, Any, Union
import sqlite3
import re


def validate_latex(content: str) -> str:
    """è‡ªåŠ¨ä¿®å¤å¸¸è§ LaTeX è¯­æ³•é”™è¯¯"""
    # é—­åˆæœªå…³é—­çš„å…¬å¼å—
    content = re.sub(r"\\\[(.*?)(?=\\])", r"\[\1\]", content, flags=re.DOTALL)

    # ä¿®æ­£å˜é‡ä¸‹æ ‡é”™è¯¯ï¼ˆå¦‚ a_r -> a_5ï¼‰
    content = re.sub(r"a_([^0-9{])", r"a_{5}", content)

    # ç§»é™¤å¤šä½™åˆ†éš”ç¬¦
    content = content.replace("\#\#", "")  # å¤„ç†ç¤ºä¾‹ä¸­çš„ ##4 ç­‰æ ‡è®°

    return content


# åœ¨è¿”å›å›ç­”å‰è°ƒç”¨

# åˆå§‹åŒ–é…ç½®

# é…ç½®å…¨å±€å‚æ•°
API_KEY = "ragflow-Q0NDI5ODllMTNiNTExZjBhMWNjMDI0Mm"
BASE_URL = "http://workspace.featurize.cn:27455"
# BASE_URL = "http://127.0.0.1"
CHAT_ID = "f14d1f6a259811f098e60242ac120006"
DEFAULT_USERNAME = "admin"
DEFAULT_PASSWORD = "123456"
LOCAL_ENVIRONMENT = True

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

doc_list = gr.DataFrame(
    value=pd.DataFrame(columns=["ID", "æ–‡ä»¶å", "çŠ¶æ€", "ä¸Šä¼ æ—¶é—´"]),
    type="pandas",
    label="æ–‡æ¡£åˆ—è¡¨",
    interactive=True,
)


def create_db():
    # åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    conn = sqlite3.connect("users.db")
    cursor = conn.cursor()

    # åˆ›å»ºç”¨æˆ·è¡¨
    cursor.execute(
        """
    CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT UNIQUE NOT NULL,
        password TEXT NOT NULL
    )
    """
    )

    conn.commit()
    conn.close()


class ParserConfig(BaseModel):
    chunk_token_num: Optional[int] = None
    delimiter: Optional[str] = None
    html4excel: Optional[bool] = None
    layout_recognize: Optional[bool] = None
    raptor: Optional[Dict] = None
    entity_types: Optional[List[str]] = None


class DataSet(BaseModel):
    id: str
    name: str
    avatar: Optional[str] = Field(default="")  # å…è®¸ç©ºå€¼å¹¶è®¾ç½®é»˜è®¤å€¼
    description: Optional[str] = Field(default="")
    embedding_model: str
    permission: str
    chunk_method: str
    parser_config: dict
    create_time: str


class Document(BaseModel):
    id: str
    display_name: str
    chunk_method: str
    parser_config: Optional[ParserConfig] = None


class RAGFlow:
    def __init__(self, api_key: str, base_url: str, chat_id: str):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/")
        self.chat_id = chat_id
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        self.sdk = OfficialRAGFlow(api_key=api_key, base_url=base_url)

    def _get_question_by_id(self, question_id: str) -> tuple:
        """è¿”å› (é—®é¢˜å†…å®¹, ç›¸å…³æ–‡æ¡£åˆ—è¡¨)"""
        try:
            cleaned_id = question_id.strip().replace("-", "_")
            keyword = f"problem_id: {cleaned_id}"
            dataset = self.sdk.list_datasets(name="é«˜ä¸­ä»£æ•°æ•°æ®é›†")[0]
            related_docs = []
            question_content = ""

            for document in dataset.list_documents():
                chunks = document.list_chunks(keywords=keyword)
                if chunks:
                    related_docs.append(document.name)
                    # ä»ç¬¬ä¸€ä¸ªåŒ¹é…çš„chunkæå–é—®é¢˜å†…å®¹
                    if not question_content:
                        clean_content = self.clean_html(chunks[0].content)
                        question_content = self._extract_question_from_content(
                            clean_content
                        )

            return question_content, related_docs
        except Exception as e:
            logger.error(f"è·å–é—®é¢˜å¤±è´¥: {e}")
            return "", []

    def clean_html(self, content: str) -> str:
        # å»æ‰HTMLæ ‡ç­¾
        clean_content = re.sub(r"<.*?>", "", content)
        return clean_content

    def _extract_question_from_content(self, content: str) -> str:
        try:
            clean_content = self.clean_html(content)  # æ¸…ç†HTMLæ ‡ç­¾
            # æ›´é€šç”¨çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é… `question` åçš„å†…å®¹
            match = re.search(r"[Qq]uestion\s*(.*?)(?:[ï¼›;ã€‚]|$)", clean_content)
            if match:
                return match.group(1).strip()
        except Exception as e:
            logger.warning(f"æå– question å¤±è´¥: {e}")
        return ""

    def get_reference_files(self, question_id: str) -> list:
        """è¿”å› [é—®é¢˜å†…å®¹ï¼Œç›¸å…³æ–‡æ¡£ï¼Œæ£€ç´¢ç»“æœ]"""
        # è·å–é—®é¢˜å†…å®¹å’Œå…³è”æ–‡æ¡£
        question_content, related_docs = self._get_question_by_id(question_id)

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_chunks = self.retrieve(
            question=question_content,  # ä½¿ç”¨æå–çš„é—®é¢˜å†…å®¹è¿›è¡Œæ£€ç´¢
            dataset_ids=["03d5fbe4259311f0870b0242ac120006"],
        )

        # æ„å»ºç»“æœè¡¨æ ¼
        file_scores = {}
        for chunk in retrieved_chunks:
            file_name = (
                chunk.get("document_name", "")
                if isinstance(chunk, dict)
                else getattr(chunk, "document_name", "")
            )
            score = (
                chunk.get("score", 0)
                if isinstance(chunk, dict)
                else getattr(chunk, "score", 0)
            )
            if file_name:  # æ·»åŠ æ–‡ä»¶åéç©ºæ ¡éªŒ
                file_scores[file_name] = max(file_scores.get(file_name, 0), score)

        # æŒ‰åˆ†æ•°æ’åºåä»…ä¿ç•™æ–‡ä»¶å
        sorted_files = [
            name
            for name, _ in sorted(
                file_scores.items(), key=lambda x: x[1], reverse=True
            )[
                :10
            ]  # å±•ç¤ºå‰10ä¸ªç»“æœ
        ]

        # æ„å»ºäºŒç»´æ•°ç»„æ ¼å¼ï¼ˆGradio DataFrameè¦æ±‚ï¼‰
        table_data = [[name] for name in sorted_files]

        return [
            ", ".join(related_docs) if related_docs else "æœªæ‰¾åˆ°ç›´æ¥å…³è”æ–‡æ¡£",
            table_data,
        ]

    def retrieve(
        self,
        question: str,
        dataset_ids: list[str],
        document_ids: list[str] = None,
        page: int = 1,
        page_size: int = 30,
        similarity_threshold: float = 0.2,
        vector_similarity_weight: float = 0.3,
        top_k: int = 1024,
        rerank_id: str = None,
        keyword: bool = False,
    ) -> list[dict]:
        processed_chunks = []  # åˆå§‹åŒ– processed_chunks ä»¥é˜²æ­¢ UnboundLocalError
        try:
            # è°ƒç”¨ SDK çš„ retrieve æ–¹æ³•
            chunks = self.sdk.retrieve(
                question=question,
                dataset_ids=dataset_ids,
                document_ids=document_ids,
                page=page,
                page_size=page_size,
                similarity_threshold=similarity_threshold,
                vector_similarity_weight=vector_similarity_weight,
                top_k=top_k,
                rerank_id=rerank_id,
                keyword=keyword,
            )

            if chunks:
                first_chunk = chunks[0]
                print("Chunkå¯¹è±¡ç±»å‹:", type(first_chunk))
                print("Chunkå¯¹è±¡å±æ€§:", dir(first_chunk))
                if hasattr(first_chunk, "content"):
                    print("å†…å®¹ç¤ºä¾‹:", first_chunk.content[:50])

            # è½¬æ¢ Chunk å¯¹è±¡ä¸ºå­—å…¸
            for chunk in chunks:
                # ç¡®ä¿ chunk.content æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å… .lower() é”™è¯¯
                chunk_content = (
                    str(chunk.content)
                    if not isinstance(chunk.content, str)
                    else chunk.content
                )

                # ç›´æ¥é€šè¿‡å¯¹è±¡è®¿é—®å±æ€§
                file_name = getattr(
                    chunk, "document_name", "æœªçŸ¥æ–‡ä»¶"
                )  # ä½¿ç”¨ getattr è®¿é—® document_name å±æ€§

                chunk_data = {
                    "content": chunk_content,
                    "score": getattr(chunk, "score", 0.0),
                    "metadata": {
                        "file_name": file_name,  # ä½¿ç”¨ä» chunk ä¸­è·å–çš„æ–‡ä»¶å
                        "problem_id": self._parse_problem_id(
                            chunk_content
                        ),  # ä¼ å…¥å·²å¤„ç†çš„ chunk_content
                    },
                }
                processed_chunks.append(chunk_data)

            return processed_chunks

        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥ï¼š{str(e)}")

            # å¼‚å¸¸æ—¶è¿”å›éƒ¨åˆ†å·²å¤„ç†çš„ç»“æœ
            return [
                {
                    "åºå·": idx + 1,
                    "æ–‡ä»¶å": chunk_data["metadata"]["file_name"],
                    "å†…å®¹æ‘˜è¦": chunk_data["content"][:50] + "...",  # è¿”å›å†…å®¹æ‘˜è¦
                    "ç›¸å…³åº¦": chunk_data["score"],
                    "é—®é¢˜ID": chunk_data["metadata"]["problem_id"],
                }
                for idx, chunk_data in enumerate(processed_chunks)
            ]

    def _parse_problem_id(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–é—®é¢˜ID"""
        match = re.search(r"problem_id\s*[:ï¼š]\s*(\d+)", content)
        return match.group(1) if match else ""

    def chat_completions(
        self,
        messages: List[Dict],
        model: str = "qwen2.5:3b",  # æŒ‡å®šé»˜è®¤æ¨¡å‹
        max_tokens: Optional[int] = None,
        temperature: float = 0.7,
        stream: bool = False,
    ) -> Dict:
        """èŠå¤©è¡¥å…¨æ¥å£"""
        endpoint = f"/api/v1/chats_openai/{self.chat_id}/chat/completions"
        url = f"{self.base_url}{endpoint}"

        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream,
        }
        if max_tokens:
            payload["max_tokens"] = max_tokens

        try:
            response = requests.post(
                url, headers=self.headers, json=payload, stream=stream, timeout=30
            )
            response.raise_for_status()

            if stream:
                return self._handle_stream_response(response)
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"Chat completions error: {str(e)}")
            raise

    def _handle_stream_response(self, response) -> Generator[Dict, None, None]:
        """å¤„ç†æµå¼å“åº”"""
        for line in response.iter_lines():
            if line:
                decoded_line = line.decode("utf-8")
                if decoded_line.startswith("data: "):
                    chunk = decoded_line[6:]
                    if chunk == "[DONE]":
                        break
                    try:
                        yield json.loads(chunk)
                    except json.JSONDecodeError:
                        continue

    def get_related_questions(self, question: str, history: list = None) -> List[str]:
        """è‡ªä¸»ç”Ÿæˆç›¸å…³é—®é¢˜ï¼ˆé€šè¿‡å¤§æ¨¡å‹ï¼‰"""
        try:
            # æ„é€ æç¤ºè¯æ¨¡æ¿
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜æ¨èå¼•æ“ã€‚æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²å’Œæœ€æ–°é—®é¢˜ï¼Œç”Ÿæˆ3ä¸ªç”¨æˆ·å¯èƒ½å…³å¿ƒçš„åç»­é—®é¢˜ã€‚
    è¦æ±‚ï¼š
    1. é—®é¢˜éœ€åŸºäºä¸Šä¸‹æ–‡ä¸”å¼€æ”¾å¯è®¨è®º
    2. ä½¿ç”¨ä¸­æ–‡æé—®
    3. æ¯ä¸ªé—®é¢˜ç”¨æ•°å­—ç¼–å·å¼€å¤´
    4. ä¸è¦ä½¿ç”¨Markdownæ ¼å¼

    ç¤ºä¾‹ï¼š
    ç”¨æˆ·é—®ï¼šæœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ
    ç›¸å…³é—®é¢˜ï¼š
    1. ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
    2. å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•æœ‰å“ªäº›ï¼Ÿ
    3. å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ

    å½“å‰å¯¹è¯å†å²ï¼ˆæœ€è¿‘3è½®ï¼‰ï¼š
    {self._format_history(history[-3:] if history else [])}

    æœ€æ–°é—®é¢˜ï¼š{question}
    ç›¸å…³é—®é¢˜ï¼š"""

            # è°ƒç”¨èŠå¤©æ¥å£
            response = self.chat_completions(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,  # æé«˜åˆ›é€ æ€§
                max_tokens=300,
            )

            # è§£æç”Ÿæˆç»“æœ
            return self._parse_questions(response["choices"][0]["message"]["content"])

        except Exception as e:
            logger.error(f"ç”Ÿæˆç›¸å…³é—®é¢˜å¤±è´¥ï¼š{str(e)}")
            return []

    def _format_history(self, history: list) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²ï¼Œåªå¤„ç† dict ç±»å‹çš„å†å²è®°å½•"""
        result = []
        for item in history:
            # item å¿…é¡»æ˜¯ dict ä¸”åŒ…å« 'role' å’Œ 'content'
            if isinstance(item, dict) and "role" in item and "content" in item:
                result.append(f"{item['role']}ï¼š{item['content']}")
            else:
                logger.warning(f"è·³è¿‡æ— æ•ˆå†å²é¡¹ï¼š{item}")
        return "\n".join(result)

    def _parse_questions(self, text: str) -> List[str]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬"""
        questions = []
        for line in text.split("\n"):
            line = line.strip()
            # æ”¯æŒå¤šç§ç¼–å·æ ¼å¼ï¼š1. 1) â€¢ ç­‰
            cleaned = re.sub(r"^(\d+[\.\)]?|[\â€¢\-])\s*", "", line)
            if 10 <= len(cleaned) <= 100:  # è¿‡æ»¤æœ‰æ•ˆé—®é¢˜
                questions.append(cleaned)
        return questions[:3]  # æœ€å¤šè¿”å›3ä¸ª

    # å‚è€ƒæ–‡ä»¶æ¥å£
    # æ•°æ®é›†ç®¡ç†ï¼ˆå®Œæ•´å®ç°ï¼‰
    def create_dataset(
        self,
        name: str,
        chunk_method: str = "naive",
        embedding_model: str = "nomic-embed-text:latest",
        description: str = "",  # æ–°å¢æè¿°å‚æ•°
        permission: str = "me",  # æ–°å¢æƒé™å‚æ•°
        **kwargs,
    ) -> DataSet:
        """åˆ›å»ºæ•°æ®é›†"""
        payload = {
            "name": name,
            "avatar": kwargs.get("avatar", ""),
            "description": description,  # ä½¿ç”¨ä¼ å…¥å‚æ•°
            "embedding_model": embedding_model,
            "permission": permission,  # ä½¿ç”¨ä¼ å…¥å‚æ•°
            "chunk_method": chunk_method,
            "parser_config": self._get_parser_config(chunk_method, kwargs),
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/v1/datasets", headers=self.headers, json=payload
            )
            response.raise_for_status()

            # æå–å®é™…æ•°æ®å­—æ®µ
            response_data = response.json().get("data", {})
            if not response_data:
                raise ValueError("æœåŠ¡ç«¯è¿”å›æ•°æ®ä¸ºç©º")

            return DataSet(
                id=response_data["id"],
                name=response_data["name"],
                embedding_model=response_data["embedding_model"],
                permission=response_data["permission"],
                chunk_method=response_data["chunk_method"],
                parser_config=response_data["parser_config"],
                create_time=datetime.fromtimestamp(
                    response_data["create_time"] / 1000
                ).isoformat(),
                avatar=response_data.get("avatar", ""),
                description=response_data.get("description", ""),
            )
        except requests.exceptions.RequestException as e:
            logger.error(f"Create dataset error: {str(e)}")
            raise

    def _get_parser_config(self, chunk_method: str, params: Dict) -> Dict:
        """ç”Ÿæˆè§£æå™¨é…ç½®"""
        config_map = {
            "naive": {
                "chunk_token_num": 128,
                "delimiter": "\\n",
                "html4excel": False,
                "layout_recognize": True,
                "raptor": {"user_raptor": False},
            },
            "knowledge_graph": {
                "chunk_token_num": 128,
                "delimiter": "\\n",
                "entity_types": ["organization", "person", "location", "event", "time"],
            },
            # å…¶ä»–åˆ†å—æ–¹æ³•çš„é»˜è®¤é…ç½®...
        }
        return params.get("parser_config", config_map.get(chunk_method, {}))

    def delete_datasets(self, dataset_ids):
        """åˆ é™¤æ•°æ®é›†"""
        url = f"{self.base_url}/api/v1/datasets"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        # è¯·æ±‚ä½“æ ¼å¼
        payload = {"ids": dataset_ids}

        # å‘é€DELETEè¯·æ±‚
        response = requests.delete(url, headers=headers, json=payload)

        # åˆ¤æ–­æ˜¯å¦åˆ é™¤æˆåŠŸ
        if response.status_code == 200:
            data = response.json()
            if data.get("code") == 0:
                return True  # åˆ é™¤æˆåŠŸ
            else:
                raise Exception(f"åˆ é™¤å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
        else:
            raise Exception(f"è¯·æ±‚å¤±è´¥: {response.status_code}, {response.text}")

    def update_dataset(self, dataset_id: str, update_data: Dict) -> DataSet:
        """æ›´æ–°æ•°æ®é›†é…ç½®"""
        try:
            response = requests.put(
                f"{self.base_url}/api/v1/datasets/{dataset_id}",
                headers=self.headers,
                json=update_data,
            )
            response.raise_for_status()
            return DataSet(**response.json()["data"])
        except requests.exceptions.RequestException as e:
            logger.error(f"Update dataset error: {str(e)}")
            raise

    def get_dataset_info(self, dataset_id: str) -> dict:
        """è·å–å•ä¸ªæ•°æ®é›†è¯¦æƒ…ï¼ˆä¿®å¤dataç±»å‹æ£€æŸ¥ï¼‰"""
        try:
            response = requests.get(
                f"{self.base_url}/api/v1/datasets/{dataset_id}", headers=self.headers
            )
            response.raise_for_status()
            response_json = response.json()

            # æ·»åŠ å“åº”ç»“æ„éªŒè¯
            if "data" not in response_json:
                logger.error("å“åº”ç¼ºå°‘dataå­—æ®µ")
                return {}

            data = response_json["data"]

            # ç±»å‹æ£€æŸ¥
            if not isinstance(data, dict):
                logger.error(f"æ•°æ®æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›å­—å…¸ï¼Œå®é™…å¾—åˆ°ï¼š{type(data)}")
                return {}

            return {
                "name": data.get("name", ""),
                "description": data.get("description", ""),
                "embedding_model": data.get("embedding_model", ""),
                "chunk_method": data.get("chunk_method", ""),
            }
        except Exception as e:
            logger.error(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def _delete_selected_datasets(self, dataset_table):
        """ä»è¡¨æ ¼ä¸­æå–é€‰ä¸­çš„æ•°æ®é›†å¹¶è°ƒç”¨åç«¯æ¥å£åˆ é™¤"""
        try:
            # è¡¨å¤´æ˜¯ ["é€‰ä¸­", "ID", "åç§°", "æè¿°", "åµŒå…¥æ¨¡å‹", "åˆ›å»ºæ—¶é—´"]
            selected_ids = [
                row[1]  # IDåœ¨ç¬¬2åˆ—ï¼ˆç´¢å¼•1ï¼‰
                for row in dataset_table
                if str(row[0]).strip().lower() in ["true", "1", "yes"]  # é€‰ä¸­åˆ—
            ]
            if not selected_ids:
                raise gr.Error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¦åˆ é™¤çš„æ•°æ®é›†ã€‚")

            self.delete_datasets(selected_ids)
            return self._refresh_datasets()
        except Exception as e:
            logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {e}")
            raise gr.Error(f"åˆ é™¤å¤±è´¥ï¼š{e}")

    def wait_for_document_ready(
        self, dataset_id: str, document_id: str, timeout: int = 300
    ) -> bool:
        """ç­‰å¾…æ–‡æ¡£çŠ¶æ€å˜æˆ UNSTARTï¼Œå¯ä»¥å¼€å§‹è§£æ"""
        for i in range(timeout):
            docs = self.list_documents(dataset_id, document_id=document_id)["docs"]
            if not docs:
                logger.debug(f"æ–‡æ¡£{document_id}è¿˜æ²¡æŸ¥åˆ°ï¼Œç»§ç»­ç­‰å¾…...")
                time.sleep(1)
                continue
            run_status = docs[0].get("run", "")
            logger.debug(f"å½“å‰æ–‡æ¡£{document_id} runçŠ¶æ€: {run_status}")
            print(f"Timeout left: {timeout - i} second(s).")
            if run_status in ("UNSTART", "DONE"):
                return True
            time.sleep(1)
        logger.error(f"æ–‡æ¡£{document_id}åœ¨{timeout}ç§’å†…æœªå‡†å¤‡å¥½(runçŠ¶æ€={run_status})")
        return False

    def list_datasets(
        self,
        page: int = 1,
        page_size: int = 30,
        orderby: str = "create_time",
        desc: bool = True,
        id: str = None,
        name: str = None,
    ) -> List[DataSet]:
        """åˆ—å‡ºæ•°æ®é›†"""
        params = {
            "page": page,
            "page_size": page_size,
            "orderby": orderby,
            "desc": str(desc).lower(),
            "id": id,
            "name": name,
        }

        try:
            response = requests.get(
                f"{self.base_url}/api/v1/datasets",
                headers=self.headers,
                params={k: v for k, v in params.items() if v is not None},
            )
            response.raise_for_status()
            return [
                DataSet(
                    **{
                        **item,
                        "create_time": datetime.fromtimestamp(
                            item["create_time"] / 1000
                        ).isoformat(),  # è½¬æ¢æ—¶é—´æˆ³
                        "avatar": item.get("avatar", ""),
                        "description": item.get("description", ""),
                    }
                )
                for item in response.json().get("data", [])
            ]
        except requests.exceptions.RequestException as e:
            logger.error(f"List datasets error: {str(e)}")
            raise

    def _refresh_datasets(self):
        """æ‹‰å–æ•°æ®é›†å¹¶è¿”å›å±•ç¤ºç»“æ„"""
        try:
            datasets = self.list_datasets()
            table_data = []
            for ds in datasets:
                table_data.append(
                    [
                        False,  # é»˜è®¤æœªé€‰ä¸­
                        ds.id,
                        ds.name,
                        ds.description,
                        ds.embedding_model,
                        ds.create_time,
                    ]
                )
            return table_data
        except Exception as e:
            logger.error(f"åˆ·æ–°æ•°æ®é›†å¤±è´¥: {e}")
            raise gr.Error(f"åˆ·æ–°å¤±è´¥: {e}")

    def update_dataset(self, dataset_id: str, update_data: Dict) -> DataSet:
        """æ›´æ–°æ•°æ®é›†ï¼ˆä¿®å¤create_timeå¤„ç†ï¼‰"""
        try:
            response = requests.put(
                f"{self.base_url}/api/v1/datasets/{dataset_id}",
                headers=self.headers,
                json=update_data,
            )
            response.raise_for_status()
            response_json = response.json()

            # éªŒè¯å“åº”ç»“æ„
            if "data" not in response_json:
                logger.error("å“åº”ç¼ºå°‘dataå­—æ®µ")
                raise ValueError("æ— æ•ˆçš„APIå“åº”")

            data = response_json["data"]

            # å¤„ç†å¯èƒ½ç¼ºå¤±çš„å­—æ®µ
            create_time = data.get("create_time", datetime.now().timestamp() * 1000)
            data["create_time"] = datetime.fromtimestamp(create_time / 1000).isoformat()

            return DataSet(**data)
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®é›†å¤±è´¥: {str(e)}")
            raise

    def upload_documents(
        self, dataset_id: str, files: List[Tuple[str, bytes]]
    ) -> List[dict]:
        try:
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            logger.debug(f"â–¼â–¼â–¼ å¼€å§‹ä¸Šä¼ æ–‡æ¡£ â–¼â–¼â–¼")
            logger.debug(f"ç›®æ ‡æ•°æ®é›†: {dataset_id}")
            logger.debug(f"æ–‡ä»¶æ•°é‡: {len(files)}")

            for i, (filename, content) in enumerate(files):
                logger.debug(
                    f"æ–‡ä»¶#{i + 1}: {filename} ({len(content)} bytes) | å‰16å­—èŠ‚: {content[:16].hex()}"
                )

            # æ„å»ºè¯·æ±‚
            file_fields = [
                ("files", (filename, content, "application/octet-stream"))
                for filename, content in files
            ]

            # è®°å½•åŸå§‹è¯·æ±‚ä¿¡æ¯
            logger.debug(
                f"è¯·æ±‚URL: {self.base_url}/api/v1/datasets/{dataset_id}/documents"
            )
            logger.debug(
                f"è¯·æ±‚å¤´: { {'Authorization': f'Bearer [REDACTED]'} }"
            )  # éšè—çœŸå®API Key

            response = requests.post(
                f"{self.base_url}/api/v1/datasets/{dataset_id}/documents",
                headers={"Authorization": f"Bearer {self.api_key}"},
                files=file_fields,
                timeout=30,
            )

            # è®°å½•å“åº”è¯¦æƒ…
            logger.debug(f"â–²â–²â–² ä¸Šä¼ å“åº” â–²â–²â–²")
            logger.debug(f"çŠ¶æ€ç : {response.status_code}")
            logger.debug(f"å“åº”å¤´: {dict(response.headers)}")
            logger.debug(f"å“åº”å†…å®¹: {response.text[:500]}...")  # æˆªæ–­é•¿å†…å®¹

            response.raise_for_status()
            return response.json().get("data", [])
        except Exception as e:
            logger.exception("ä¸Šä¼ å‘ç”Ÿè‡´å‘½é”™è¯¯")
            raise gr.Error(f"ä¸Šä¼ å¤±è´¥: {str(e)}")

    def list_documents(
        self,
        dataset_id: str,
        page: int = 1,
        page_size: int = 30,
        keywords: str = None,
        document_id: str = None,
    ) -> dict:
        """æŸ¥è¯¢æ–‡æ¡£ï¼ˆæ ¹æ®API 2.4ï¼‰"""
        try:
            params = {
                "page": page,
                "page_size": page_size,
                "keywords": keywords,
                "id": document_id,
            }

            response = requests.get(
                f"{self.base_url}/api/v1/datasets/{dataset_id}/documents",
                headers=self.headers,
                params={k: v for k, v in params.items() if v is not None},
            )
            response.raise_for_status()

            # è½¬æ¢æ—¶é—´æ ¼å¼å’ŒçŠ¶æ€
            data = response.json().get("data", {})

            for doc in data.get("docs", []):

                doc["create_time"] = datetime.fromtimestamp(
                    doc["create_time"] / 1000
                ).strftime("%Y-%m-%d %H:%M:%S")
                doc["status"] = self._map_status(doc.get("run", ""))

            return {"docs": data.get("docs", []), "total": data.get("total", 0)}
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            return {"docs": [], "total": 0}

    def _map_status(self, run_status: str) -> str:
        """æ˜ å°„æ–‡æ¡£çŠ¶æ€"""
        status_map = {
            "UNSTART": "ğŸŸ¡ æœªå¼€å§‹",
            "RUNNING": "ğŸ”„ è§£æä¸­",
            "DONE": "âœ… å·²å®Œæˆ",
            "FAILED": "âŒ å¤±è´¥",
        }
        return status_map.get(run_status, "â“ æœªçŸ¥çŠ¶æ€")

    def delete_documents(self, dataset_id: str, document_ids: List[str]) -> bool:
        """åˆ é™¤æ–‡æ¡£ï¼ˆæ ¹æ®API 2.5ï¼‰"""
        try:
            response = requests.delete(
                f"{self.base_url}/api/v1/datasets/{dataset_id}/documents",
                headers=self.headers,
                json={"ids": document_ids},
            )
            return response.json().get("code", 1) == 0
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    def parse_documents(self, dataset_id: str, document_ids: List[str]) -> bool:
        """è§£ææ–‡æ¡£ï¼ˆæ ¹æ®API 2.6ï¼‰"""
        try:
            response = requests.post(
                f"{self.base_url}/api/v1/datasets/{dataset_id}/chunks",
                headers=self.headers,
                json={"document_ids": document_ids},
            )
            # DYC æ–°åŠ å…¥è°ƒè¯•ä¿¡æ¯æŸ¥çœ‹é”™è¯¯
            logger.info(f"è§£ææ¥å£è¿”å›: {response.status_code}, {response.text}")
            if response.status_code == 200:
                logger.info("è§£æè¯·æ±‚æˆåŠŸ")
                return True
            return response.json().get("code", 1) == 0
        except Exception as e:
            logger.error(f"è§£ææ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

    # æ–‡æ¡£ç®¡ç†åŠŸèƒ½

    def cancel_parse(self, dataset_id: str, document_ids: List[str]) -> None:
        """å–æ¶ˆæ–‡æ¡£è§£æ"""
        try:
            response = requests.post(
                f"{self.base_url}/api/v1/datasets/{dataset_id}/chunks/cancel",
                headers=self.headers,
                json={"document_ids": document_ids},
            )
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            logger.error(f"Cancel parsing error: {str(e)}")
            raise

    def _update_doc_list(self, dataset_id):
        """æ›´æ–°æ–‡æ¡£åˆ—è¡¨"""
        if not dataset_id:
            return pd.DataFrame(columns=["ID", "æ–‡ä»¶å", "çŠ¶æ€", "ä¸Šä¼ æ—¶é—´"])

        documents = self.list_documents(dataset_id)

        return pd.DataFrame(
            [
                {
                    "ID": doc.id,
                    "æ–‡ä»¶å": doc.display_name,
                    "çŠ¶æ€": doc.parser_config.get("run", "æœªçŸ¥"),
                    "ä¸Šä¼ æ—¶é—´": self._format_timestamp(
                        doc.parser_config.get("create_time")
                    ),
                }
                for doc in documents
            ]
        )


custom_css = f"""
/* ä¸»è‰²è°ƒ */
:root {{
  --primary: #F8F9FA;         /* ä¸»èƒŒæ™¯ */
  --secondary: #FFFFFF;      /* å¡ç‰‡èƒŒæ™¯ */
  --accent: #6C63FF;         /* å¼ºè°ƒè‰²-ç´«ç½—å…° */
  --text: #2D3436;           /* æ­£æ–‡é¢œè‰² */
  --gradient: linear-gradient(135deg, #F8F9FA 0%, #E9ECEF 100%);
}}

* {{
    font-family: "Cambria Math", "Times New Roman", serif;
}}

body {{
    font-family: "Cambria Math", "Times New Roman", serif;
}}

/* ä¸­æ–‡å­—ä½“è®¾ç½® */
@font-face {{
    font-family: "STSong";
    src: local("åæ–‡ä¸­å®‹");
}}

html, body {{
    font-family: "Cambria Math", "Times New Roman", "STSong", serif;
}}

/* ä¸­æ–‡æ–‡æœ¬ä½¿ç”¨åæ–‡ä¸­å®‹ */
* {{
    font-family: "STSong", "Cambria Math", "Times New Roman", serif;
}}

/* æ•°å­¦å…¬å¼å­—ä½“è®¾ç½®ä¸º Cambria Math */
.mathjax {{
    font-family: "Cambria Math", "Times New Roman", serif;
}}

.main-container {{
  background: var(--gradient);
  min-height: 100vh;
  padding: 2rem;
}}

.contact-panel {{
  background: var(--secondary);
  border-radius: 12px;
  box-shadow: 0 4px 20px rgba(108, 99, 255, 0.1);
  padding: 1.5rem;
  margin-right: 1rem;
}}

.chat-panel {{
  background: var(--secondary);
  border-radius: 12px;
  box-shadow: 0 4px 30px rgba(108, 99, 255, 0.05);
  padding: 2rem;
}}

.message-flow {{
  min-height: 70vh;
  max-height: 70vh;
  overflow-y: auto;
  padding: 1rem;
  background: var(--primary);
  border-radius: 8px;
}}

.input-area {{
  margin-top: 1.5rem;
  position: relative;
}}

.message-bubble.user {{
  background: var(--accent);
  color: white;
  border-radius: 18px 18px 4px 18px;
}}

.message-bubble.bot {{
  background: var(--secondary);
  border: 1px solid #E9ECEF;
  border-radius: 18px 18px 18px 4px;
}}

button:hover {{
  transform: translateY(-2px);
  box-shadow: 0 5px 15px rgba(108, 99, 255, 0.2);
}}

.input-area::before {{
  content: "";
  position: absolute;
  top: -15px;
  left: 0;
  right: 0;
  height: 2px;
  background: linear-gradient(90deg, var(--accent) 0%, rgba(108,99,255,0) 100%);
}}

#problem-id-input {{
    background: #f8f9fa;
    border: 2px solid #6C63FF;
    border-radius: 8px;
    padding: 12px;
}}

button#problem-id-search {{
    transition: all 0.3s ease;
}}
button#problem-id-search:hover {{
    transform: scale(1.05);
    box-shadow: 0 4px 15px rgba(108, 99, 255, 0.3);
}}

.result-table {{
    margin-top: 20px;
    border-radius: 8px !important;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1) !important;
}}
.result-table th {{
    background: #6C63FF !important;
    color: white !important;
}}
.sidebar {{
    background-image: url("https://raw.githubusercontent.com/Yu-chen-Deng/yu-chen-deng.github.io/refs/heads/main/Du/img/sidebar-bg.png");
}}
"""


class ChatInterface:
    def __init__(self, rag_client: RAGFlow):
        self.client = rag_client
        self.custom_css = custom_css
        self.login_interface = None
        self.main_interface = None
        self.guest_interface = None
        self.register_interface = None
        self.user_state = None
        self.login_status = None

    def create_interface(self):
        with gr.Blocks(
            theme=gr.themes.Monochrome(), css=self.custom_css, title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
        ) as demo:
            self.user_state = gr.State(value={"logged_in": False, "role": None})

            # === ç™»å½•ç•Œé¢ ===
            with gr.Column(visible=True) as self.login_interface:
                gr.Markdown("# ğŸ¤– KAG - ç™»å½•")

                username = gr.Textbox(label="ç”¨æˆ·å", placeholder="è¯·è¾“å…¥ç”¨æˆ·å")
                password = gr.Textbox(
                    label="å¯†ç ", type="password", placeholder="è¯·è¾“å…¥å¯†ç "
                )

                with gr.Row():
                    login_btn = gr.Button("ç™»å½•", variant="primary")
                    guest_btn = gr.Button("æ¸¸å®¢ç™»å½•", variant="secondary")
                    register_btn = gr.Button("æ³¨å†Œ", variant="secondary")

                self.login_status = gr.Markdown("")

            # === æ³¨å†Œç•Œé¢ ===
            with gr.Column(visible=False) as self.register_interface:
                gr.Markdown("# âœ¨ æ³¨å†Œæ–°è´¦å·")

                new_username = gr.Textbox(label="ç”¨æˆ·å", placeholder="è¯·è¾“å…¥æ–°ç”¨æˆ·å")
                new_password = gr.Textbox(
                    label="å¯†ç ", type="password", placeholder="è¯·è¾“å…¥æ–°å¯†ç "
                )
                confirm_password = gr.Textbox(
                    label="ç¡®è®¤å¯†ç ", type="password", placeholder="è¯·ç¡®è®¤å¯†ç "
                )

                register_status = gr.Markdown("")

                # å®šä¹‰æ³¨å†Œç”¨æˆ·å‡½æ•°
                def register_user(username, password, confirm_password):
                    if password != confirm_password:
                        return gr.update(), "âŒ ä¸¤æ¬¡å¯†ç ä¸ä¸€è‡´ã€‚"

                    conn = sqlite3.connect("users.db")
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT * FROM users WHERE username = ?", (username,)
                    )
                    if cursor.fetchone():
                        conn.close()
                        return gr.update(), "âŒ ç”¨æˆ·åå·²å­˜åœ¨ï¼Œè¯·é€‰æ‹©å…¶ä»–ç”¨æˆ·åã€‚"

                    cursor.execute(
                        "INSERT INTO users (username, password) VALUES (?, ?)",
                        (username, password),
                    )
                    conn.commit()
                    conn.close()
                    return gr.update(visible=True), "âœ… æ³¨å†ŒæˆåŠŸï¼Œæ‚¨å¯ä»¥ç™»å½•äº†ï¼"

                register_btn.click(
                    fn=register_user,
                    inputs=[new_username, new_password, confirm_password],
                    outputs=[self.login_interface, register_status],
                )

                back_to_login_btn = gr.Button("è¿”å›ç™»å½•", variant="secondary")
                back_to_login_btn.click(
                    fn=lambda: (
                        gr.update(visible=True),  # æ˜¾ç¤ºç™»å½•ç•Œé¢
                        gr.update(visible=False),  # éšè—æ³¨å†Œç•Œé¢
                    ),
                    inputs=[],
                    outputs=[self.login_interface, self.register_interface],
                )

            # === ä¸»ç•Œé¢ ===
            with gr.Column(visible=False) as self.main_interface:
                gr.Markdown("# ğŸ¤– KAG")

                with gr.Sidebar(position="left", elem_classes=["sidebar"]) as sidebar:
                    gr.Markdown("# ğŸ§­ åŠŸèƒ½å¯¼èˆª")
                    tab_select = gr.Radio(
                        choices=["æ™ºèƒ½é—®ç­”", "å‚è€ƒæ–‡ä»¶æŸ¥è¯¢", "æ•°æ®é›†ç®¡ç†", "æ–‡æ¡£ç®¡ç†"],
                        label="è¯·é€‰æ‹©æ¨¡å—",
                        value="æ™ºèƒ½é—®ç­”",
                    )

                # å­é¡µé¢å®¹å™¨
                chat_tab = gr.Column(visible=True)
                reference_tab = gr.Column(visible=False)
                dataset_tab = gr.Column(visible=False)
                document_tab = gr.Column(visible=False)

                with chat_tab:
                    self._build_chat_interface()
                with reference_tab:
                    self._build_reference_interface()
                with dataset_tab:
                    self._build_dataset_interface()
                with document_tab:
                    self._build_document_interface()

                # Tabåˆ‡æ¢
                def switch_tab(selected):
                    return (
                        gr.update(visible=selected == "æ™ºèƒ½é—®ç­”"),
                        gr.update(visible=selected == "å‚è€ƒæ–‡ä»¶æŸ¥è¯¢"),
                        gr.update(visible=selected == "æ•°æ®é›†ç®¡ç†"),
                        gr.update(visible=selected == "æ–‡æ¡£ç®¡ç†"),
                    )

                tab_select.change(
                    fn=switch_tab,
                    inputs=tab_select,
                    outputs=[chat_tab, reference_tab, dataset_tab, document_tab],
                )

            # === ç®€åŒ–ç‰ˆä¸»ç•Œé¢ï¼ˆæ¸¸å®¢ï¼‰ ===
            with gr.Column(visible=False) as self.guest_interface:
                gr.Markdown("# ğŸ™‹ æ¸¸å®¢æ™ºèƒ½é—®ç­”")
                self._build_chat_interface(minimal=True)  # ç”¨ä¸åŒå‚æ•°åŒºåˆ†ç²¾ç®€ç‰ˆ

            # === ç™»å½•å›è°ƒé€»è¾‘ ===
            def admin_login(username, password):
                if username == "admin" and password == "123456":
                    return (
                        gr.update(visible=False),  # éšè—ç™»å½•
                        gr.update(visible=True),  # æ˜¾ç¤ºä¸»ç•Œé¢
                        gr.update(visible=False),  # éšè—æ¸¸å®¢ç•Œé¢
                        {"logged_in": True, "role": "admin"},
                        "âœ… ç™»å½•æˆåŠŸï¼Œæ¬¢è¿ç®¡ç†å‘˜ï¼",
                    )

                if not username or not password:
                    return (
                        gr.update(),
                        gr.update(),
                        gr.update(),
                        gr.update(),
                        "âŒ ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©ºã€‚",
                    )

                # å¦‚æœæ˜¯æ•°æ®åº“ä¸­çš„ç”¨æˆ·ï¼ŒéªŒè¯ç”¨æˆ·åå’Œå¯†ç 
                conn = sqlite3.connect("users.db")
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT * FROM users WHERE username = ? AND password = ?",
                    (username, password),
                )
                user = cursor.fetchone()

                conn.close()

                if user:
                    # å¦‚æœç”¨æˆ·åå’Œå¯†ç åŒ¹é…ï¼Œç™»å½•æˆåŠŸ
                    return (
                        gr.update(visible=False),  # éšè—ç™»å½•ç•Œé¢
                        gr.update(visible=True),  # æ˜¾ç¤ºä¸»ç•Œé¢
                        gr.update(visible=False),  # éšè—æ¸¸å®¢ç•Œé¢
                        {"logged_in": True, "role": "user"},  # è®¾ç½®è§’è‰²ä¸ºæ™®é€šç”¨æˆ·
                        f"âœ… ç™»å½•æˆåŠŸï¼Œæ¬¢è¿ {username}ï¼",
                    )
                else:
                    # å¦‚æœç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                    return (
                        gr.update(),
                        gr.update(),
                        gr.update(),
                        gr.update(),
                        "âŒ ç™»å½•å¤±è´¥ï¼Œç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ã€‚",
                    )

            def guest_login():
                return (
                    gr.update(visible=False),  # éšè—ç™»å½•
                    gr.update(visible=False),  # éšè—ä¸»ç•Œé¢
                    gr.update(visible=True),  # æ˜¾ç¤ºæ¸¸å®¢ç•Œé¢
                    {"logged_in": True, "role": "guest"},
                    "ğŸ‰ å·²ä»¥æ¸¸å®¢èº«ä»½ç™»å½•",
                )

            login_btn.click(
                fn=admin_login,
                inputs=[username, password],
                outputs=[
                    self.login_interface,
                    self.main_interface,
                    self.guest_interface,
                    self.user_state,
                    self.login_status,
                ],
            )

            guest_btn.click(
                fn=guest_login,
                inputs=[],
                outputs=[
                    self.login_interface,
                    self.main_interface,
                    self.guest_interface,
                    self.user_state,
                    self.login_status,
                ],
            )

            register_btn.click(
                fn=lambda: (
                    gr.update(visible=False),  # éšè—ç™»å½•ç•Œé¢
                    gr.update(visible=True),  # æ˜¾ç¤ºæ³¨å†Œç•Œé¢
                ),
                inputs=[],
                outputs=[self.login_interface, self.register_interface],
            )

        return demo

    def _handle_return_to_login(self):
        """å¤„ç†è¿”å›ç™»å½•ç•Œé¢çš„é€»è¾‘"""
        return (
            gr.update(visible=True),  # æ˜¾ç¤ºç™»å½•ç•Œé¢
            gr.update(visible=False),  # éšè—ä¸»ç•Œé¢
            gr.update(visible=False),  # éšè—æ¸¸å®¢ç•Œé¢
            {"logged_in": False, "role": None},  # é‡ç½®ç”¨æˆ·çŠ¶æ€
            "",
        )

    def _build_reference_interface(self):
        """é‡æ„åçš„å‚è€ƒæ–‡ä»¶æŸ¥è¯¢ç•Œé¢"""
        with gr.Row():
            with gr.Column(scale=4):
                # æ–°å¢é—®é¢˜IDè¾“å…¥åŒºåŸŸ
                with gr.Group():
                    problem_id_input = gr.Textbox(
                        label="è¯·è¾“å…¥é—®é¢˜ID",
                        placeholder="ä¾‹å¦‚ï¼š1",
                        max_lines=1,
                        elem_id="problem-id-input",
                    )
                    search_btn = gr.Button("å¼€å§‹æ£€ç´¢", variant="primary")

                # ç»“æœå±•ç¤ºè¡¨æ ¼
                result_table = gr.DataFrame(
                    headers=["æ–‡æ¡£åç§°"],
                    datatype=["str"],
                    interactive=False,
                    elem_classes=["result-table"],
                    label="ç›¸å…³æ–‡æ¡£åˆ—è¡¨",
                )

            # å³ä¾§ä¿¡æ¯å±•ç¤ºï¼ˆå¯é€‰ï¼‰
            with gr.Column(scale=1):
                gr.Markdown("### ä½¿ç”¨è¯´æ˜")
                gr.Markdown(
                    """
                1. åœ¨è¾“å…¥æ¡†è¾“å…¥å®Œæ•´çš„é—®é¢˜ID
                2. ç‚¹å‡»ã€Œå¼€å§‹æ£€ç´¢ã€æŒ‰é’®
                3. ç³»ç»Ÿå°†å±•ç¤ºä¸è¯¥é—®é¢˜ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£
                """
                )

        # äº‹ä»¶ç»‘å®š
        search_btn.click(
            fn=self.client.get_reference_files,
            inputs=problem_id_input,
            outputs=result_table,
        )
        problem_id_input.submit(
            fn=self.client.get_reference_files,
            inputs=problem_id_input,
            outputs=result_table,
        )

    def _build_chat_interface(self, minimal=False):
        """è°ƒæ•´åçš„èŠå¤©ç•Œé¢ï¼ˆç§»é™¤åŸå³ä¾§é—®é¢˜IDè¾“å…¥ï¼‰"""
        if minimal:
            with gr.Row():

                # å·¦ä¾§èŠå¤©åŒºåŸŸï¼ˆç§»é™¤é—®é¢˜IDç›¸å…³ç»„ä»¶ï¼‰
                with gr.Column(scale=3):
                    with gr.Row():
                        return_btn = gr.Button(
                            "è¿”å›ç™»å½•", variant="secondary", size="sm"
                        )
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        elem_classes=["chat-history"],
                        height=600,
                        type="messages",
                        latex_delimiters=[
                            {"left": "$$", "right": "$$", "display": True},
                            {"left": r"\(", "right": r"\)", "display": False},
                            {"left": r"\[", "right": r"\]", "display": True},
                        ],
                        render_markdown=True,
                    )
                    msg = gr.Textbox(placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=3)

                    with gr.Row():
                        submit_btn = gr.Button("å‘é€", variant="primary")
                        clear_btn = gr.Button("æ¸…ç©ºå†å²")

                    with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                        model_select = gr.Dropdown(
                            choices=["qwen2.5:3b"], value="qwen2.5:3b"
                        )
                        max_tokens = gr.Slider(100, 1000, value=300, label="æœ€å¤§é•¿åº¦")
                        temperature = gr.Slider(0, 2, value=0.7, label="éšæœºæ€§")

                    submit_btn.click(
                        self._respond,
                        inputs=[msg, chatbot, model_select, max_tokens, temperature],
                        outputs=[msg, chatbot],  # ç§»é™¤reference_filesè¾“å‡º
                    )
                    clear_btn.click(lambda: ([], [], [], []), outputs=[chatbot, msg])
                    return_btn.click(
                        fn=self._handle_return_to_login,  # è¿”å›ç™»å½•ç•Œé¢
                        inputs=[],
                        outputs=[
                            self.login_interface,
                            self.main_interface,
                            self.guest_interface,
                            self.user_state,
                            self.login_status,
                        ],
                    )

        else:
            with gr.Row():
                # å·¦ä¾§èŠå¤©åŒºåŸŸï¼ˆç§»é™¤é—®é¢˜IDç›¸å…³ç»„ä»¶ï¼‰
                with gr.Column(scale=3):
                    with gr.Row():
                        return_btn = gr.Button(
                            "è¿”å›ç™»å½•", variant="secondary", size="sm"
                        )
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        elem_classes=["chat-history"],
                        height=600,
                        type="messages",
                        latex_delimiters=[
                            {"left": "$$", "right": "$$", "display": True},
                            {"left": r"\(", "right": r"\)", "display": False},
                            {"left": r"\[", "right": r"\]", "display": True},
                        ],
                        render_markdown=True,
                    )
                    msg = gr.Textbox(placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=3)

                    with gr.Row():
                        submit_btn = gr.Button("å‘é€", variant="primary")
                        clear_btn = gr.Button("æ¸…ç©ºå†å²")

                    with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                        model_select = gr.Dropdown(
                            choices=["qwen2.5:14b"], value="qwen2.5:14b"
                        )
                        max_tokens = gr.Slider(100, 1000, value=300, label="æœ€å¤§é•¿åº¦")
                        temperature = gr.Slider(0, 2, value=0.7, label="éšæœºæ€§")

                # å³ä¾§é¢æ¿ç°åœ¨åªæ˜¾ç¤ºç›¸å…³é—®é¢˜
                with gr.Column(scale=1, elem_classes=["info-panel"]):
                    related_questions = gr.JSON(label="ç›¸å…³é—®é¢˜æ¨è")

            # è°ƒæ•´äº‹ä»¶ç»‘å®š
            submit_btn.click(
                self._respond,
                inputs=[msg, chatbot, model_select, max_tokens, temperature],
                outputs=[msg, chatbot, related_questions],  # ç§»é™¤reference_filesè¾“å‡º
            )
            clear_btn.click(
                lambda: ([], [], [], []), outputs=[chatbot, related_questions, msg]
            )

    def _respond(self, message, history, model, max_tokens, temperature):
        try:
            # ç”Ÿæˆå›ç­”
            answer = self._get_answer(message, history, model, max_tokens, temperature)

            # ç”Ÿæˆç›¸å…³é—®é¢˜
            related_questions = self.client.get_related_questions(message, history)

            # è½¬æ¢ä¸ºGradioè¦æ±‚çš„æ¶ˆæ¯æ ¼å¼ï¼ˆç»Ÿä¸€å­—å…¸æ ¼å¼ï¼‰
            new_history = history + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": answer},
            ]

            return "", new_history, related_questions
        except Exception as e:  # ä¿®å¤å†’å·ç¼ºå¤±
            error_msg = f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}"
            logger.exception(error_msg)
            return "", history, []

    def _get_answer(self, message, history, model, max_tokens, temperature):
        """è·å–èŠå¤©å›å¤"""
        try:
            response = self.client.chat_completions(
                messages=self._format_messages(message, history),
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
            )

            logger.info(f"LLM è¿”å›: {response}")  # åŠ è¿™ä¸€è¡ŒæŸ¥çœ‹çœŸå®ç»“æ„
            #  æ£€æŸ¥ç»“æ„
            if not response or "choices" not in response:
                raise ValueError(f"å“åº”ä¸­æ—  'choices': {response}")

            return response["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"è¯·æ±‚å¼‚å¸¸ï¼š{str(e)}")
            return "âš ï¸ æŠ±æ­‰ï¼Œæ¨¡å‹å“åº”å¤±è´¥ã€‚"

    # æ—§æ ¼å¼é€‚é…æ–¹å¼ï¼ˆå¦‚æœ history æ˜¯ [(é—®é¢˜, ç­”æ¡ˆ)]ï¼‰
    def _format_messages(self, message, history):
        """å…¼å®¹æ—§ç‰ˆå…ƒç»„æ ¼å¼å’Œå­—å…¸æ ¼å¼"""
        messages = []
        for item in history:
            if isinstance(item, dict) and "role" in item and "content" in item:
                messages.append(item)  # ç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼
            elif isinstance(item, (tuple, list)) and len(item) >= 2:
                # å°†æ—§ç‰ˆå…ƒç»„è½¬ä¸ºå­—å…¸
                messages.extend(
                    [
                        {"role": "user", "content": item[0]},
                        {"role": "assistant", "content": item[1]},
                    ]
                )
        messages.append({"role": "user", "content": message})
        return messages

    # def _delete_selected_datasets(self, dataset_table):
    #     """ä»è¡¨æ ¼ä¸­æå–é€‰ä¸­çš„æ•°æ®é›†å¹¶è°ƒç”¨åç«¯æ¥å£åˆ é™¤"""
    #     print(dataset_table)
    #     try:
    #         # è¡¨å¤´æ˜¯ ["é€‰ä¸­", "ID", "åç§°", "æè¿°", "åµŒå…¥æ¨¡å‹", "åˆ›å»ºæ—¶é—´"]
    #         selected_ids = [
    #             row[1]  # IDåœ¨ç¬¬2åˆ—ï¼ˆç´¢å¼•1ï¼‰
    #             for row in dataset_table
    #             if str(row[0]).strip().lower() in ["true", "1", "yes"]  # é€‰ä¸­åˆ—
    #         ]
    #         if not selected_ids:
    #             raise gr.Error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¦åˆ é™¤çš„æ•°æ®é›†ã€‚")

    #         self.delete_datasets(selected_ids)
    #         return self._refresh_datasets()
    #     except Exception as e:
    #         logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {e}")
    #         raise gr.Error(f"åˆ é™¤å¤±è´¥ï¼š{e}")

    def wait_for_document_ready(
        self, dataset_id: str, document_id: str, timeout: int = 30
    ) -> bool:
        """ç­‰å¾…æ–‡æ¡£çŠ¶æ€å˜æˆ UNSTARTï¼Œå¯ä»¥å¼€å§‹è§£æ"""
        for i in range(timeout):
            docs = self.client.list_documents(dataset_id, document_id=document_id)[
                "docs"
            ]
            if not docs:
                logger.debug(f"æ–‡æ¡£{document_id}è¿˜æ²¡æŸ¥åˆ°ï¼Œç»§ç»­ç­‰å¾…...")
                time.sleep(1)
                continue
            run_status = docs[0].get("run", "")
            logger.debug(f"å½“å‰æ–‡æ¡£{document_id} runçŠ¶æ€: {run_status}")
            print(f"Timeout left: {timeout - i} second(s).")
            if run_status in ("UNSTART", "DONE"):
                return True
            time.sleep(1)
        logger.error(f"æ–‡æ¡£{document_id}åœ¨{timeout}ç§’å†…æœªå‡†å¤‡å¥½(runçŠ¶æ€={run_status})")
        return False

    def _delete_selected_datasets(self, selected_names):
        # é€šè¿‡åç§°æ˜ å°„åˆ°å®Œæ•´æ•°æ®é›†å¯¹è±¡
        all_datasets = self.client.list_datasets()
        selected_datasets = [ds for ds in all_datasets if ds.name in selected_names]

        # åŸåˆ é™¤é€»è¾‘ï¼ˆå‡è®¾éœ€è¦æ•°æ®é›†IDï¼‰
        dataset_ids = [ds.id for ds in selected_datasets]
        print(dataset_ids)
        self.client.delete_datasets(dataset_ids)

        # è¿”å›æ›´æ–°åçš„åˆ—è¡¨
        return self._refresh_datasets()

    def _refresh_datasets(self):
        """æ‹‰å–æ•°æ®é›†å¹¶è¿”å›å±•ç¤ºç»“æ„"""
        try:
            datasets = self.client.list_datasets()
            table_data = []
            for ds in datasets:
                table_data.append(
                    [
                        True,  # é»˜è®¤æœªé€‰ä¸­
                        ds.id,
                        ds.name,
                        ds.description,
                        ds.embedding_model,
                        ds.create_time,
                    ]
                )
            return table_data
        except Exception as e:
            logger.error(f"åˆ·æ–°æ•°æ®é›†å¤±è´¥: {e}")
            raise gr.Error(f"åˆ·æ–°å¤±è´¥: {e}")

    def _refresh_dataset_options(self):
        """è¿”å›æ‰€æœ‰æ•°æ®é›†ï¼Œç”¨äºåˆ·æ–°ä¸‹æ‹‰åˆ—è¡¨æˆ–å¤é€‰æ¡†"""
        try:
            datasets = self.client.list_datasets()
            return [f"{ds.name}ï¼ˆ{ds.id}ï¼‰" for ds in datasets if ds.id]
        except Exception as e:
            logger.error(f"åˆ·æ–°æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")
            return self._get_dataset_choices()

    def _get_dataset_choices(self):
        datasets = self.vector_db.list_datasets()
        return [f"{ds['name']}ï¼ˆ{ds['id']}ï¼‰" for ds in datasets]

    def _delete_selected_datasets_by_ids(self, selected_list):
        """é€šè¿‡å¤é€‰æ¡†é€‰æ‹©çš„æ˜¾ç¤ºæ–‡æœ¬åˆ é™¤å¯¹åº”æ•°æ®é›†"""
        try:
            ids = [
                item.split("ï¼ˆ")[-1].strip("ï¼‰") for item in selected_list
            ]  # æå– ID
            if ids:
                self.client.delete_datasets(ids)
            return self._refresh_dataset_options()
        except Exception as e:
            logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {str(e)}")
            return []

    def _create_dataset_and_refresh(
        self, name, description, chunk_method, embedding_model
    ):
        """åˆ›å»ºæ•°æ®é›†å¹¶åˆ·æ–°åˆ—è¡¨"""
        try:
            self.client.create_dataset(
                name=name,
                description=description,
                chunk_method=chunk_method,
                embedding_model=embedding_model,
            )
        except Exception as e:
            logger.error(f"åˆ›å»ºå¤±è´¥: {str(e)}")
        return self._refresh_dataset_options()

    def _build_dataset_interface(self):
        """æ„å»ºæ•°æ®é›†ç®¡ç†ç•Œé¢"""
        with gr.Row():
            # åˆ›å»ºæ•°æ®é›†è¡¨å•
            with gr.Column(scale=2):
                with gr.Column(variant="panel"):
                    gr.Markdown("## åˆ›å»ºæ–°æ•°æ®é›†")
                    ds_name = gr.Textbox(label="æ•°æ®é›†åç§°")
                    ds_description = gr.Textbox(label="æ•°æ®é›†æè¿°")
                    chunk_method = gr.Dropdown(
                        choices=["naive", "knowledge_graph"],
                        label="åˆ†å—æ–¹æ³•",
                        value="naive",
                    )
                    embedding_model = gr.Dropdown(
                        choices=["nomic-embed-text:latest"],
                        label="åµŒå…¥æ¨¡å‹",
                        value="nomic-embed-text:latest",
                    )
                    create_btn = gr.Button("åˆ›å»ºæ•°æ®é›†", variant="primary")

                # æ•°æ®é›†åˆ—è¡¨åŠæ“ä½œ
                with gr.Column(scale=3):

                    dataset_list = gr.DataFrame(  # æ•°æ®é›†åˆ—è¡¨
                        label="æ•°æ®é›†åˆ—è¡¨",
                        headers=["é€‰ä¸­", "ID", "åç§°", "æè¿°", "åµŒå…¥æ¨¡å‹", "åˆ›å»ºæ—¶é—´"],
                        datatype=["bool", "str", "str", "str", "str", "str"],
                        interactive=True,
                        row_count=5,
                        col_count=6,
                    )

                    # æ“ä½œæŒ‰é’®ç»„
                    with gr.Column(scale=0.5):
                        refresh_btn = gr.Button("åˆ·æ–°æ•°æ®é›†åˆ—è¡¨")
                        # åŸ refresh_btn åæ·»åŠ ï¼š
                        dataset_options = [
                            ds.name for ds in self.client.list_datasets()
                        ]
                        dataset_selector = gr.CheckboxGroup(
                            choices=dataset_options, label="é€‰æ‹©è¦åˆ é™¤çš„æ•°æ®é›†"
                        )
                        delete_btn = gr.Button("åˆ é™¤é€‰ä¸­çš„æ•°æ®é›†", variant="stop")

            # æ•°æ®é›†ä¿®æ”¹è¡¨å•
            with gr.Column(scale=2):
                gr.Markdown("## ä¿®æ”¹æ•°æ®é›†")
                dataset_id_input = gr.Textbox(label="æ•°æ®é›†ID")
                with gr.Row():

                    update_btn = gr.Button("æäº¤ä¿®æ”¹", variant="primary")
                update_name = gr.Textbox(label="æ–°åç§°")
                update_description = gr.Textbox(label="æ–°æè¿°")
                update_embedding_model = gr.Dropdown(
                    label="åµŒå…¥æ¨¡å‹",
                    choices=["nomic-embed-text:latest"],
                    value="nomic-embed-text:latest",
                )
                update_chunk_method = gr.Dropdown(
                    label="åˆ†å—æ–¹æ³•",
                    choices=["naive", "knowledge_graph"],
                    value="naive",
                )

        # äº‹ä»¶ç»‘å®š
        create_btn.click(
            self._create_dataset_and_refresh,
            inputs=[ds_name, ds_description, chunk_method, embedding_model],
            outputs=[dataset_list],
        )

        refresh_btn.click(
            self._refresh_datasets, inputs=[], outputs=[dataset_list]  # åˆ·æ–°æ•°æ®é›†åˆ—è¡¨
        )

        delete_btn.click(
            self._delete_selected_datasets,
            inputs=[dataset_selector],
            outputs=[dataset_list],
        )

        update_btn.click(
            self._update_dataset_info,
            inputs=[
                dataset_id_input,
                update_name,
                update_description,
                update_embedding_model,
                update_chunk_method,
            ],
            outputs=[dataset_list],
        )

    def _create_dataset_and_refresh(
        self, name, description, chunk_method, embedding_model
    ):
        """åˆ›å»ºæ•°æ®é›†å¹¶åˆ·æ–°åˆ—è¡¨"""
        self.client.create_dataset(
            name=name,
            description=description,
            chunk_method=chunk_method,
            embedding_model=embedding_model,
        )
        return self._format_datasets()

    def _refresh_datasets(self, page=1, page_size=30):
        return self._format_datasets()

    def _format_datasets(self):
        datasets = self.client.list_datasets()
        data = []
        for ds in datasets:
            data.append(
                [
                    False,  # é€‰ä¸­
                    ds.id,
                    ds.name,
                    ds.description or "",
                    ds.embedding_model,
                    ds.create_time,
                ]
            )
        return pd.DataFrame(
            data, columns=["é€‰ä¸­", "ID", "åç§°", "æè¿°", "åµŒå…¥æ¨¡å‹", "åˆ›å»ºæ—¶é—´"]
        )

    # åˆ·æ–°æ•°æ®é›†åˆ—è¡¨

    def _load_dataset_info(self, dataset_id):
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯åˆ°è¡¨å•"""
        info = self.client.get_dataset_info(dataset_id)
        return [
            info.get("name", ""),
            info.get("description", ""),
            info.get("embedding_model", ""),
            info.get("chunk_method", ""),
        ]

    def _docs_to_table(self, docs: List[dict]):
        return pd.DataFrame(
            [
                {
                    "ID": doc.get("id"),
                    "æ–‡ä»¶å": doc.get("name") or doc.get("display_name"),
                    "çŠ¶æ€": doc.get("status", "æœªçŸ¥"),
                    "åˆ›å»ºæ—¶é—´": doc.get("create_time"),
                    "å¤§å°": round(doc.get("size", 0) / 1024, 2),
                }
                for doc in docs
            ]
        )

    def _build_document_interface(self):
        """é‡æ„åçš„æ–‡æ¡£ç®¡ç†ç•Œé¢"""
        with gr.Column():
            with gr.Row():
                # æ•°æ®é›†é€‰æ‹©
                dataset_selector = gr.Dropdown(
                    label="ğŸ“ é€‰æ‹©çŸ¥è¯†åº“",
                    choices=[],  # åˆå§‹ç©ºï¼Œæˆ–è€…ç»™ä¸€æ‰¹é»˜è®¤æ•°æ®
                    interactive=True,
                    scale=2,
                )

                # æ“ä½œæŒ‰é’®ç»„
                with gr.Row(scale=1):
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡æ¡£", variant="secondary")
                    refresh_btn2 = gr.Button("ğŸ”„ åˆ·æ–°çŸ¥è¯†åº“", variant="secondary")
                    parse_btn = gr.Button("ğŸ”§ è§£ææ–‡æ¡£", variant="primary")
                    delete_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£", variant="stop")

            # æ–‡æ¡£ä¸Šä¼ åŒºåŸŸ
            with gr.Column(variant="panel"):
                gr.Markdown("## ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
                file_upload = gr.File(
                    label="é€‰æ‹©æ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
                    file_count="multiple",
                    file_types=[".pdf", ".docx", ".xlsx", ".txt", ".md"],
                    height=150,
                )

                upload_btn = gr.Button("ğŸš€ å¼€å§‹ä¸Šä¼ ", variant="primary")

            # æ–‡æ¡£åˆ—è¡¨
            doc_list = gr.DataFrame(
                headers=["ID", "æ–‡ä»¶å", "çŠ¶æ€", "åˆ›å»ºæ—¶é—´", "å¤§å°"],
                datatype=["str", "str", "str", "str", "number"],
                interactive=False,
                wrap=True,
            )

            # ========== å°è£…å¼‚å¸¸å¤„ç†é€»è¾‘ ==========
            def safe_upload(ds_id, files):
                try:
                    return self._handle_upload(ds_id, files)
                except Exception as e:
                    raise gr.Error(f"ä¸Šä¼ å¤±è´¥ï¼š{str(e)}")

            def safe_delete(ds_id, df):
                try:
                    return self._handle_delete(ds_id, df)
                except Exception as e:
                    raise gr.Error(f"åˆ é™¤å¤±è´¥ï¼š{str(e)}")

            # ========== äº‹ä»¶ç»‘å®š ==========
            dataset_selector.change(
                fn=lambda ds_id: self._docs_to_table(
                    self.client.list_documents(ds_id)["docs"]
                ),
                inputs=dataset_selector,
                outputs=doc_list,
            ).then(lambda: gr.Info("å·²åˆ‡æ¢çŸ¥è¯†åº“"), None, None)

            upload_btn.click(
                fn=safe_upload, inputs=[dataset_selector, file_upload], outputs=doc_list
            ).then(lambda: gr.Info("ä¸Šä¼ æˆåŠŸï¼"), None, None)

            def refresh_doc_list(ds_id):
                docs = self.client.list_documents(ds_id)["docs"]
                return self._docs_to_table(docs)

            def refresh_dataset_options():
                options = self._get_dataset_options()
                return gr.update(choices=options)

            refresh_btn.click(
                fn=refresh_doc_list,
                inputs=dataset_selector,
                outputs=doc_list,
            ).then(lambda: gr.Info("å·²åˆ·æ–°æ–‡æ¡£åˆ—è¡¨"), None, None)
            refresh_btn2.click(
                fn=refresh_dataset_options,
                inputs=[],
                outputs=[dataset_selector],
            )
            delete_btn.click(
                fn=safe_delete, inputs=[dataset_selector, doc_list], outputs=doc_list
            ).then(lambda: gr.Info("åˆ é™¤æˆåŠŸï¼"), None, None)

            parse_btn.click(
                fn=lambda ds_id, df: self._handle_parse(ds_id, df),
                inputs=[dataset_selector, doc_list],
                outputs=doc_list,
            ).then(lambda: gr.Info("è§£æå·²å¼€å§‹"), None, None)

    def _handle_delete(self, dataset_id, doc_df):
        """å¤„ç†åˆ é™¤é€»è¾‘"""
        if doc_df.empty:
            raise gr.Error("è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡æ¡£")

        doc_ids = doc_df["ID"].tolist()
        if self.client.delete_documents(dataset_id, doc_ids):
            return self.client.list_documents(dataset_id)["docs"]
        raise gr.Error("åˆ é™¤æ“ä½œå¤±è´¥")

    def _handle_parse(self, dataset_id, doc_df):
        """å¤„ç†è§£æé€»è¾‘"""
        if doc_df.empty:
            raise gr.Error("è¯·å…ˆé€‰æ‹©è¦è§£æçš„æ–‡æ¡£")

        doc_ids = doc_df["ID"].tolist()

        # DYCæ–°å¢ï¼š æ–‡æ¡£çŠ¶æ€æ£€æŸ¥ï¼Œç­‰å¾…æ‰€æœ‰æ–‡æ¡£å˜ä¸º UNSTART
        for doc_id in doc_ids:
            ready = self.client.wait_for_document_ready(dataset_id, doc_id, timeout=30)
            if not ready:
                raise gr.Error(f"æ–‡æ¡£ {doc_id} æœªå‡†å¤‡å¥½ï¼Œæ— æ³•è§£æ")

        if self.client.parse_documents(dataset_id, doc_ids):
            return self.client.list_documents(dataset_id)["docs"]
        raise gr.Error("è§£ææ“ä½œå¤±è´¥")

    def _load_dataset_info(self, dataset_id):
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯åˆ°è¡¨å•"""
        info = self.client.get_dataset_info(dataset_id)
        return [
            info.get("name", ""),
            info.get("description", ""),
            info.get("embedding_model", ""),
            info.get("chunk_method", ""),
        ]

    def _update_dataset_info(
        self, dataset_id, name, description, embedding_model, chunk_method
    ):
        """æäº¤æ•°æ®é›†ä¿®æ”¹"""
        self.client.update_dataset(
            dataset_id=dataset_id,
            update_data={
                "name": name,
                "description": description,
                "embedding_model": embedding_model,
                "chunk_method": chunk_method,
            },
        )

    def _get_dataset_options(self):
        """è·å–æ•°æ®é›†ä¸‹æ‹‰é€‰é¡¹"""
        datasets = self.client.list_datasets()
        # æ·»åŠ ç©ºå€¼è¿‡æ»¤
        return [(ds.name or "æœªå‘½åæ•°æ®é›†", ds.id) for ds in datasets if ds.id]

    def _update_doc_list(self, dataset_id):
        """æ›´æ–°æ–‡æ¡£åˆ—è¡¨"""
        if not dataset_id:
            return []
        return self.client.list_documents(dataset_id)

    # def _handle_upload(self, dataset_id, files):
    #     """å¤„ç†ä¸Šä¼ é€»è¾‘"""
    #     if not dataset_id:
    #         raise gr.Error("è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“")

    #     try:
    #         # è½¬æ¢Gradioæ–‡ä»¶å¯¹è±¡ä¸ºFileStorage
    #         uploaded_files = [(file.name, file) for file in files]

    #         # è°ƒç”¨API
    #         result = self.client.upload_documents(dataset_id, uploaded_files)
    #         return self.client.list_documents(dataset_id)["docs"]
    #     except Exception as e:
    #         logger.error(f"ä¸Šä¼ å¼‚å¸¸ï¼š{str(e)}")
    #         raise gr.Error("æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–ç½‘ç»œè¿æ¥")

    # DYCä¿®å¤
    def _handle_upload(self, dataset_id, files):
        if not dataset_id:
            raise gr.Error("è¯·å…ˆé€‰æ‹©çŸ¥è¯†åº“")

        try:
            multipart_files = []
            open_files = []  # ä¿å­˜æ‰“å¼€çš„æ–‡ä»¶å¯¹è±¡ï¼Œä¸Šä¼ åå†å…³é—­
            for file_path in files:
                f = open(file_path, "rb")
                open_files.append(f)  # ä¿å­˜å¼•ç”¨ä»¥é˜²æ­¢æå‰å…³é—­
                filename = os.path.basename(file_path)
                multipart_files.append(
                    ("file", (filename, f, self._detect_mime(filename)))
                )

            response = requests.post(
                url=f"{self.client.base_url}/api/v1/datasets/{dataset_id}/documents",
                headers={
                    k: v
                    for k, v in self.client.headers.items()
                    if k.lower() != "content-type"
                },
                files=multipart_files,
            )
            response.raise_for_status()

            return self.client.list_documents(dataset_id)["docs"]

        except Exception as e:
            logger.exception("ä¸Šä¼ å¤„ç†å¼‚å¸¸ï¼š")
            raise gr.Error(f"ä¸Šä¼ å¤±è´¥ï¼š{str(e)}")

        finally:
            # ä¸è®ºæˆåŠŸå¤±è´¥éƒ½å…³é—­æ–‡ä»¶
            for f in open_files:
                f.close()

    def _detect_mime(self, filename):
        """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        from mimetypes import guess_type

        return guess_type(filename)[0] or "application/octet-stream"

    # æ•°æ®é›†äº‹ä»¶å¤„ç†å‡½æ•°


if __name__ == "__main__":
    # åœ¨ç¨‹åºå¯åŠ¨æ—¶åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    create_db()

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    rag_client = RAGFlow(api_key=API_KEY, base_url=BASE_URL, chat_id=CHAT_ID)

    # åˆ›å»ºç•Œé¢
    interface = ChatInterface(rag_client)
    demo = interface.create_interface()
    demo.launch(server_port=7860, pwa=True, allowed_paths=["./"])
